# ============================================================================
# DEÄERLENDÄ°RME MODÃœLÃœ - evaluate.py
# ============================================================================
# Bu modÃ¼l, eÄŸitilmiÅŸ modellerin performansÄ±nÄ± Ã¶lÃ§mek iÃ§in gerekli
# metrik hesaplama ve gÃ¶rselleÅŸtirme fonksiyonlarÄ±nÄ± iÃ§erir.
# ============================================================================

"""
Model DeÄŸerlendirme ModÃ¼lÃ¼

Bu modÃ¼l, makine Ã¶ÄŸrenmesi modellerinin performansÄ±nÄ± deÄŸerlendirmek
iÃ§in Ã§eÅŸitli metrikler ve gÃ¶rselleÅŸtirmeler saÄŸlar.

Hesaplanan Metrikler:
    - Accuracy (DoÄŸruluk)
    - Precision (Kesinlik)
    - Recall (DuyarlÄ±lÄ±k)
    - F1-Score
    - Confusion Matrix (KarÄ±ÅŸÄ±klÄ±k Matrisi)

Fonksiyonlar:
    - evaluate_model: Tek bir modeli deÄŸerlendirir
    - print_classification_report: DetaylÄ± sÄ±nÄ±flandÄ±rma raporu yazdÄ±rÄ±r
    - plot_confusion_matrix: KarÄ±ÅŸÄ±klÄ±k matrisini gÃ¶rselleÅŸtirir
    - compare_models: Birden fazla modelin sonuÃ§larÄ±nÄ± karÅŸÄ±laÅŸtÄ±rÄ±r
"""

import numpy as np
import matplotlib.pyplot as plt
from typing import Dict, Any, List, Optional, Tuple
from sklearn.metrics import (
    accuracy_score,
    precision_score,
    recall_score,
    f1_score,
    confusion_matrix,
    classification_report,
    roc_auc_score,
    roc_curve
)
import seaborn as sns
from .model import SatisfactionModel


def evaluate_model(
    model: SatisfactionModel,
    X_test: np.ndarray,
    y_test: np.ndarray,
    verbose: bool = True
) -> Dict[str, Any]:
    """
    EÄŸitilmiÅŸ modelin test verisi Ã¼zerindeki performansÄ±nÄ± deÄŸerlendirir.
    
    Bu fonksiyon, modelin tahminlerini alÄ±r ve Ã§eÅŸitli sÄ±nÄ±flandÄ±rma
    metriklerini hesaplar. SonuÃ§lar hem sayÄ±sal deÄŸerler hem de
    gÃ¶rsel formatta sunulabilir.
    
    Parametreler:
    ------------
    model : SatisfactionModel
        DeÄŸerlendirilecek eÄŸitilmiÅŸ model.
    
    X_test : np.ndarray
        Test Ã¶zellikleri.
    
    y_test : np.ndarray
        GerÃ§ek test etiketleri.
    
    verbose : bool, varsayÄ±lan=True
        True ise sonuÃ§lar konsola yazdÄ±rÄ±lÄ±r.
    
    DÃ¶ndÃ¼rÃ¼r:
    ---------
    Dict[str, Any]
        TÃ¼m metrikleri iÃ§eren sÃ¶zlÃ¼k:
        - 'accuracy': DoÄŸruluk skoru
        - 'precision': Kesinlik skoru
        - 'recall': DuyarlÄ±lÄ±k skoru
        - 'f1_score': F1 skoru
        - 'confusion_matrix': KarÄ±ÅŸÄ±klÄ±k matrisi
        - 'predictions': Model tahminleri
        - 'roc_auc': ROC-AUC skoru (varsa)
    
    Ã–rnek:
    ------
    >>> metrics = evaluate_model(model, X_test, y_test)
    >>> print(f"F1 Skoru: {metrics['f1_score']:.4f}")
    """
    model_name = model.get_model_name()
    
    if verbose:
        print(f"\n{'=' * 60}")
        print(f"MODEL DEÄERLENDÄ°RMESÄ°: {model_name}")
        print(f"{'=' * 60}")
    
    # Tahminleri alÄ±yoruz
    y_pred = model.predict(X_test)
    
    # Temel metrikleri hesaplÄ±yoruz
    accuracy = accuracy_score(y_test, y_pred)
    precision = precision_score(y_test, y_pred, average='binary')
    recall = recall_score(y_test, y_pred, average='binary')
    f1 = f1_score(y_test, y_pred, average='binary')
    conf_matrix = confusion_matrix(y_test, y_pred)
    
    # ROC-AUC hesaplamayÄ± deniyoruz (predict_proba varsa)
    roc_auc = None
    try:
        y_proba = model.predict_proba(X_test)[:, 1]
        roc_auc = roc_auc_score(y_test, y_proba)
    except Exception:
        pass  # predict_proba desteklenmiyorsa atla
    
    # SonuÃ§ sÃ¶zlÃ¼ÄŸÃ¼
    results = {
        'model_name': model_name,
        'accuracy': accuracy,
        'precision': precision,
        'recall': recall,
        'f1_score': f1,
        'confusion_matrix': conf_matrix,
        'predictions': y_pred,
        'roc_auc': roc_auc
    }
    
    if verbose:
        print_metrics(results)
    
    return results


def print_metrics(metrics: Dict[str, Any]) -> None:
    """
    Hesaplanan metrikleri formatlanmÄ±ÅŸ ÅŸekilde konsola yazdÄ±rÄ±r.
    
    Bu fonksiyon, evaluate_model() tarafÄ±ndan dÃ¶ndÃ¼rÃ¼len metrik
    sÃ¶zlÃ¼ÄŸÃ¼nÃ¼ alÄ±r ve okunabilir bir formatta ekrana yazdÄ±rÄ±r.
    
    Parametreler:
    ------------
    metrics : Dict[str, Any]
        Metrikleri iÃ§eren sÃ¶zlÃ¼k.
    """
    print("\nğŸ“Š PERFORMANS METRÄ°KLERÄ°:")
    print("-" * 40)
    
    # Ana metrikler
    print(f"  â€¢ DoÄŸruluk (Accuracy):     {metrics['accuracy']:.4f}  ({metrics['accuracy']*100:.2f}%)")
    print(f"  â€¢ Kesinlik (Precision):    {metrics['precision']:.4f}  ({metrics['precision']*100:.2f}%)")
    print(f"  â€¢ DuyarlÄ±lÄ±k (Recall):     {metrics['recall']:.4f}  ({metrics['recall']*100:.2f}%)")
    print(f"  â€¢ F1-Skoru:                {metrics['f1_score']:.4f}  ({metrics['f1_score']*100:.2f}%)")
    
    if metrics.get('roc_auc'):
        print(f"  â€¢ ROC-AUC:                 {metrics['roc_auc']:.4f}  ({metrics['roc_auc']*100:.2f}%)")
    
    # KarÄ±ÅŸÄ±klÄ±k matrisi
    cm = metrics['confusion_matrix']
    print("\nğŸ“‹ KARIÅIKLIK MATRÄ°SÄ°:")
    print("-" * 40)
    print(f"                    Tahmin")
    print(f"                  Neg    Pos")
    print(f"  GerÃ§ek  Neg  [{cm[0][0]:5d}  {cm[0][1]:5d}]")
    print(f"          Pos  [{cm[1][0]:5d}  {cm[1][1]:5d}]")
    
    # KarÄ±ÅŸÄ±klÄ±k matrisinin yorumu
    tn, fp, fn, tp = cm.ravel()
    print(f"\n  AÃ§Ä±klama:")
    print(f"    - True Negative (DoÄŸru Negatif):  {tn:,}")
    print(f"    - False Positive (YanlÄ±ÅŸ Pozitif): {fp:,}")
    print(f"    - False Negative (YanlÄ±ÅŸ Negatif): {fn:,}")
    print(f"    - True Positive (DoÄŸru Pozitif):  {tp:,}")


def print_classification_report(
    y_test: np.ndarray,
    y_pred: np.ndarray,
    target_names: Optional[List[str]] = None
) -> str:
    """
    DetaylÄ± sÄ±nÄ±flandÄ±rma raporunu yazdÄ±rÄ±r ve dÃ¶ndÃ¼rÃ¼r.
    
    Scikit-learn'Ã¼n classification_report fonksiyonunu kullanarak
    her sÄ±nÄ±f iÃ§in ayrÄ± ayrÄ± precision, recall ve f1-score deÄŸerlerini
    gÃ¶sterir.
    
    Parametreler:
    ------------
    y_test : np.ndarray
        GerÃ§ek etiketler.
    
    y_pred : np.ndarray
        Model tahminleri.
    
    target_names : List[str], opsiyonel
        SÄ±nÄ±f isimleri. VarsayÄ±lan: ['Dissatisfied', 'Satisfied']
    
    DÃ¶ndÃ¼rÃ¼r:
    ---------
    str
        FormatlanmÄ±ÅŸ sÄ±nÄ±flandÄ±rma raporu.
    """
    if target_names is None:
        target_names = ['Dissatisfied', 'Satisfied']
    
    report = classification_report(
        y_test, y_pred,
        target_names=target_names,
        digits=4
    )
    
    print("\nğŸ“‘ SINIFLANDIRMA RAPORU:")
    print("-" * 60)
    print(report)
    
    return report


def plot_confusion_matrix(
    metrics: Dict[str, Any],
    save_path: Optional[str] = None,
    figsize: Tuple[int, int] = (8, 6)
) -> None:
    """
    KarÄ±ÅŸÄ±klÄ±k matrisini Ä±sÄ± haritasÄ± olarak gÃ¶rselleÅŸtirir.
    
    Seaborn kÃ¼tÃ¼phanesi kullanÄ±larak renkli ve etiketli bir
    karÄ±ÅŸÄ±klÄ±k matrisi grafiÄŸi oluÅŸturulur.
    
    Parametreler:
    ------------
    metrics : Dict[str, Any]
        evaluate_model() tarafÄ±ndan dÃ¶ndÃ¼rÃ¼len metrik sÃ¶zlÃ¼ÄŸÃ¼.
    
    save_path : str, opsiyonel
        GrafiÄŸin kaydedileceÄŸi dosya yolu. None ise kaydedilmez.
    
    figsize : Tuple[int, int], varsayÄ±lan=(8, 6)
        Grafik boyutu (geniÅŸlik, yÃ¼kseklik) inÃ§ cinsinden.
    
    Ã–rnek:
    ------
    >>> metrics = evaluate_model(model, X_test, y_test)
    >>> plot_confusion_matrix(metrics, save_path='confusion_matrix.png')
    """
    cm = metrics['confusion_matrix']
    model_name = metrics.get('model_name', 'Model')
    
    # Grafik oluÅŸturuyoruz
    plt.figure(figsize=figsize)
    
    # IsÄ± haritasÄ± Ã§iziyoruz
    sns.heatmap(
        cm,
        annot=True,
        fmt='d',
        cmap='Blues',
        xticklabels=['Dissatisfied', 'Satisfied'],
        yticklabels=['Dissatisfied', 'Satisfied'],
        annot_kws={'size': 14}
    )
    
    plt.title(f'KarÄ±ÅŸÄ±klÄ±k Matrisi - {model_name}', fontsize=14, fontweight='bold')
    plt.xlabel('Tahmin Edilen SÄ±nÄ±f', fontsize=12)
    plt.ylabel('GerÃ§ek SÄ±nÄ±f', fontsize=12)
    
    plt.tight_layout()
    
    if save_path:
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        print(f"  âœ“ KarÄ±ÅŸÄ±klÄ±k matrisi kaydedildi: {save_path}")
    
    plt.show()


def plot_roc_curve(
    model: SatisfactionModel,
    X_test: np.ndarray,
    y_test: np.ndarray,
    save_path: Optional[str] = None,
    figsize: Tuple[int, int] = (8, 6)
) -> None:
    """
    ROC eÄŸrisini Ã§izer.
    
    Receiver Operating Characteristic (ROC) eÄŸrisi, modelin
    farklÄ± eÅŸik deÄŸerlerinde gÃ¶sterdiÄŸi performansÄ± gÃ¶rselleÅŸtirir.
    
    Parametreler:
    ------------
    model : SatisfactionModel
        DeÄŸerlendirilecek model.
    
    X_test : np.ndarray
        Test Ã¶zellikleri.
    
    y_test : np.ndarray
        GerÃ§ek test etiketleri.
    
    save_path : str, opsiyonel
        GrafiÄŸin kaydedileceÄŸi dosya yolu.
    
    figsize : Tuple[int, int], varsayÄ±lan=(8, 6)
        Grafik boyutu.
    """
    try:
        y_proba = model.predict_proba(X_test)[:, 1]
    except Exception:
        print("  ! Bu model ROC eÄŸrisi iÃ§in olasÄ±lÄ±k tahmini desteklemiyor.")
        return
    
    fpr, tpr, thresholds = roc_curve(y_test, y_proba)
    roc_auc = roc_auc_score(y_test, y_proba)
    
    plt.figure(figsize=figsize)
    
    plt.plot(fpr, tpr, color='darkorange', lw=2,
             label=f'ROC EÄŸrisi (AUC = {roc_auc:.4f})')
    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--',
             label='Rastgele SÄ±nÄ±flandÄ±rÄ±cÄ±')
    
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('YanlÄ±ÅŸ Pozitif OranÄ± (False Positive Rate)', fontsize=12)
    plt.ylabel('DoÄŸru Pozitif OranÄ± (True Positive Rate)', fontsize=12)
    plt.title(f'ROC EÄŸrisi - {model.get_model_name()}', fontsize=14, fontweight='bold')
    plt.legend(loc='lower right', fontsize=10)
    plt.grid(True, alpha=0.3)
    
    plt.tight_layout()
    
    if save_path:
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        print(f"  âœ“ ROC eÄŸrisi kaydedildi: {save_path}")
    
    plt.show()


def compare_models(
    results_list: List[Dict[str, Any]],
    save_path: Optional[str] = None
) -> None:
    """
    Birden fazla modelin sonuÃ§larÄ±nÄ± karÅŸÄ±laÅŸtÄ±rÄ±r ve gÃ¶rselleÅŸtirir.
    
    Bu fonksiyon, farklÄ± modellerin performans metriklerini
    yan yana bar grafiÄŸi olarak gÃ¶sterir.
    
    Parametreler:
    ------------
    results_list : List[Dict[str, Any]]
        Her model iÃ§in evaluate_model() sonuÃ§larÄ±nÄ±n listesi.
    
    save_path : str, opsiyonel
        GrafiÄŸin kaydedileceÄŸi dosya yolu.
    
    Ã–rnek:
    ------
    >>> results = [evaluate_model(m, X_test, y_test) for m in models]
    >>> compare_models(results)
    """
    if not results_list:
        print("KarÅŸÄ±laÅŸtÄ±rÄ±lacak sonuÃ§ bulunamadÄ±.")
        return
    
    # Model isimlerini ve metrikleri Ã§Ä±karÄ±yoruz
    model_names = [r['model_name'] for r in results_list]
    accuracies = [r['accuracy'] for r in results_list]
    precisions = [r['precision'] for r in results_list]
    recalls = [r['recall'] for r in results_list]
    f1_scores = [r['f1_score'] for r in results_list]
    
    # Bar grafiÄŸi iÃ§in veri hazÄ±rlÄ±ÄŸÄ±
    x = np.arange(len(model_names))
    width = 0.2
    
    fig, ax = plt.subplots(figsize=(12, 6))
    
    bars1 = ax.bar(x - 1.5*width, accuracies, width, label='DoÄŸruluk', color='#2ecc71')
    bars2 = ax.bar(x - 0.5*width, precisions, width, label='Kesinlik', color='#3498db')
    bars3 = ax.bar(x + 0.5*width, recalls, width, label='DuyarlÄ±lÄ±k', color='#e74c3c')
    bars4 = ax.bar(x + 1.5*width, f1_scores, width, label='F1-Skoru', color='#9b59b6')
    
    ax.set_xlabel('Model', fontsize=12)
    ax.set_ylabel('Skor', fontsize=12)
    ax.set_title('Model KarÅŸÄ±laÅŸtÄ±rmasÄ±', fontsize=14, fontweight='bold')
    ax.set_xticks(x)
    ax.set_xticklabels(model_names, fontsize=11)
    ax.legend(loc='lower right', fontsize=10)
    ax.set_ylim([0, 1.1])
    ax.grid(True, alpha=0.3, axis='y')
    
    # Bar deÄŸerlerini Ã¼zerine yazÄ±yoruz
    def add_value_labels(bars):
        for bar in bars:
            height = bar.get_height()
            ax.annotate(f'{height:.3f}',
                       xy=(bar.get_x() + bar.get_width() / 2, height),
                       xytext=(0, 3),
                       textcoords="offset points",
                       ha='center', va='bottom', fontsize=8)
    
    add_value_labels(bars1)
    add_value_labels(bars2)
    add_value_labels(bars3)
    add_value_labels(bars4)
    
    plt.tight_layout()
    
    if save_path:
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        print(f"  âœ“ KarÅŸÄ±laÅŸtÄ±rma grafiÄŸi kaydedildi: {save_path}")
    
    plt.show()
    
    # Ã–zet tablo
    print("\nğŸ“Š MODEL KARÅILAÅTIRMA TABLOSU:")
    print("-" * 70)
    print(f"{'Model':<25} {'Accuracy':<12} {'Precision':<12} {'Recall':<12} {'F1':<12}")
    print("-" * 70)
    
    for r in results_list:
        print(f"{r['model_name']:<25} {r['accuracy']:<12.4f} {r['precision']:<12.4f} "
              f"{r['recall']:<12.4f} {r['f1_score']:<12.4f}")
    
    print("-" * 70)


# Bu modÃ¼l doÄŸrudan Ã§alÄ±ÅŸtÄ±rÄ±ldÄ±ÄŸÄ±nda bilgi verir
if __name__ == "__main__":
    print("Evaluate ModÃ¼lÃ¼ - Bilgi")
    print("-" * 40)
    print("\nBu modÃ¼l model deÄŸerlendirmesi iÃ§in kullanÄ±lÄ±r.")
    print("KullanÄ±m iÃ§in main.py dosyasÄ±nÄ± Ã§alÄ±ÅŸtÄ±rÄ±n.")
